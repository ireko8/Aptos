{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "import random\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from time import time, sleep\n",
    "from math import cos, pi\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import pretrainedmodels\n",
    "import sklearn.metrics\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from albumentations import HorizontalFlip, Compose, RandomCrop, RandomContrast, Normalize, Resize, ShiftScaleRotate, VerticalFlip, Cutout, ElasticTransform, Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self, home=True):\n",
    "        self.seed = 71\n",
    "        self.batch_size = 16\n",
    "        self.accum_time = 4\n",
    "        self.all_csv = '../preprocessed/all.csv'\n",
    "        self.train_dir = '../preprocessed/train_crop/'\n",
    "        self.train_csv = '../input/train.csv'\n",
    "        self.prev_train_dir = '../previous/previous_train_crop'\n",
    "        self.test_dir = '../preprocessed/test_crop/'\n",
    "        self.test_csv = '../input/sample_submission.csv'\n",
    "        self.dup_csv = '../preprocessed/strMd5.csv'\n",
    "        self.easy_csv = '../preprocessed/easy.csv'\n",
    "        self.device_name = 'cuda:0'\n",
    "        self.weighted_sample = True\n",
    "        self.mixup_train = False\n",
    "        self.image_size = 224\n",
    "        self.n_splits = 5\n",
    "        self.fold = 0\n",
    "        self.num_epoch = 128\n",
    "        self.lr_step_epoch = 64\n",
    "        self.alpha = 1\n",
    "        self.mixup = False\n",
    "        self.init_lr = 1e-3\n",
    "        self.eta_min = 1e-6\n",
    "        self.num_workers = 16 if home else 4\n",
    "        self.classes_num = 1\n",
    "    \n",
    "conf = Config(home=True)\n",
    "assert((conf.mixup_train and conf.mixup) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    \n",
    "def load_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def count_parameter(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    lr = list()\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr.append(param_group['lr'])\n",
    "    if len(lr) == 1:\n",
    "        return lr[0]\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for kernel\n",
    "def setup(exp_name, config):\n",
    "    \"\"\"init experiment (directory setup etc...)\"\"\"\n",
    "\n",
    "    result_dir = Path(f'../result/{exp_name}/')\n",
    "    result_dir.mkdir(parents=True)\n",
    "    shutil.copy(\"main.ipynb\", result_dir)\n",
    "\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    device = torch.device(config.device_name)\n",
    "\n",
    "    log = Logger(exp_name, result_dir / 'exp.log')\n",
    "\n",
    "    log.info(\"configuration is following...\")\n",
    "    log.info(pformat(config.__dict__))\n",
    "\n",
    "    return device, log, result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \"\"\"Logging Uitlity Class for monitoring and debugging\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 log_fname,\n",
    "                 log_level=logging.INFO,\n",
    "                 custom_log_handler=None):\n",
    "\n",
    "        self.name = name\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(log_level)\n",
    "        ch = logging.FileHandler(log_fname)\n",
    "        self.logger.addHandler(ch)\n",
    "        self.logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "        if custom_log_handler:\n",
    "            if isinstance(custom_log_handler, list):\n",
    "                for handler in custom_log_handler:\n",
    "                    self.logger.addHandler(handler)\n",
    "            else:\n",
    "                self.logger.addHandler(handler)\n",
    "\n",
    "    def kiritori(self):\n",
    "        self.logger.info('-'*80)\n",
    "\n",
    "    def double_kiritori(self):\n",
    "        self.logger.info('='*80)\n",
    "\n",
    "    def space(self):\n",
    "        self.logger.info('\\n')\n",
    "\n",
    "    @contextmanager\n",
    "    def interval_timer(self, name):\n",
    "        start_time = datetime.now()\n",
    "        self.logger.info(\"\\n\")\n",
    "        self.logger.info(f\"Execution {name} start at {start_time}\")\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            end_time = datetime.now()\n",
    "            td = end_time - start_time\n",
    "            self.logger.info(f\"Execution {name} end at {end_time}\")\n",
    "            self.logger.info(f\"Execution Time : {td}\")\n",
    "            self.logger.info(\"\\n\")\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\"\n",
    "        for calling logging class attribute\n",
    "        if you call attributes of other class, raise AttributeError\n",
    "        \"\"\"\n",
    "        # self.logger.info(f\"{datetime.now()}\")\n",
    "        return getattr(self.logger, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hole_size = conf.image_size // 10\n",
    "train_transform = Compose([\n",
    "    HorizontalFlip(),\n",
    "    VerticalFlip(),\n",
    "    ShiftScaleRotate(rotate_limit=180),\n",
    "#     RandomBrightness(limit=0.2, p=0.2),\n",
    "    RandomContrast(limit=0.2, p=0.2),\n",
    "#     Resize(conf.image_size, conf.image_size),\n",
    "    RandomCrop(conf.image_size, conf.image_size),\n",
    "#     ElasticTransform(),\n",
    "#     Blur(p=0.1),\n",
    "    Cutout(max_h_size=max_hole_size, max_w_size=max_hole_size, num_holes=8, p=0.2),\n",
    "    Normalize(),\n",
    "])\n",
    "\n",
    "valid_transform = Compose([\n",
    "    Resize(conf.image_size, conf.image_size),\n",
    "    Normalize(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ratthachat/aptos-updated-preprocessing-ben-s-cropping\n",
    "def crop_image1(img, tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img > tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img, tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img > tol\n",
    "        return img[np.ix_(mask.any(1), mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img > tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3], axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "def load_ben_color(path, sigmaX=30):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = crop_image_from_gray(image)\n",
    "    image = cv2.resize(image, (conf.image_size, conf.image_size))\n",
    "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image , (0, 0) , sigmaX), -4, 128)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_num(target):\n",
    "    labels = np.zeros(conf.classes_num)\n",
    "#     labels[:(target+1)] = 1\n",
    "#     labels[target] = 1\n",
    "#     labels = target\n",
    "    labels = np.expand_dims(target, -1)\n",
    "    return labels.astype(np.float32)\n",
    "\n",
    "\n",
    "def return_path(row, fname):\n",
    "    if row.prev:\n",
    "        fpath = Path(conf.prev_train_dir) / fname\n",
    "    else:\n",
    "        fpath = Path(conf.train_dir) / fname\n",
    "    return fpath\n",
    "\n",
    "class APTOSDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 data_csv,\n",
    "                 augment=None,\n",
    "                 test=False,\n",
    "                mixup=False):\n",
    "        super().__init__()\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.data_csv = data_csv\n",
    "        self.augment = augment\n",
    "        self.test = test\n",
    "        self.mixup = mixup\n",
    "        \n",
    "    def do_mixup(self, img, label, alpha=1.):\n",
    "        index = np.random.randint(0,len(self.data_csv))\n",
    "        row = self.data_csv.loc[index]\n",
    "        fname = f\"{row.id_code}.npy\"\n",
    "#         fpath = return_path(row, fname)\n",
    "        fpath = self.root_dir / fname\n",
    "#         img2 = np.array(Image.open(fpath))\n",
    "        # img2 = load_ben_color(str(fpath))\n",
    "        img2 = np.load(fpath)\n",
    "        if self.augment:\n",
    "            img2 = self.augment(image=img2)['image']\n",
    "            img2 = np.moveaxis(img2, -1, 0)\n",
    "        \n",
    "        label2 = row.diagnosis\n",
    "        label2 = convert_num(label2)\n",
    "        \n",
    "        rate = np.random.beta(alpha, alpha)\n",
    "        img = img*rate + img2*(1-rate)\n",
    "        label = label*rate + label2*(1-rate)\n",
    "        return img, label.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_csv)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = dict()\n",
    "        row = self.data_csv.loc[index]\n",
    "        fname = f\"{row.id_code}.npy\"\n",
    "#         fpath = return_path(row, fname)\n",
    "        fpath = self.root_dir / fname\n",
    "#         image = np.array(Image.open(fpath))\n",
    "        # image = load_ben_color(str(fpath))\n",
    "        image = np.load(fpath)\n",
    "        \n",
    "        if self.augment:\n",
    "            image = self.augment(image=image)['image']\n",
    "            image = np.moveaxis(image, -1, 0)\n",
    "        \n",
    "        if self.test != \"test\":\n",
    "            label = convert_num(row.diagnosis)\n",
    "            if self.mixup and np.random.random() < 0.5:\n",
    "                image, label = self.do_mixup(image, label)\n",
    "            sample['label'] = label\n",
    "\n",
    "        sample['data'] = np.array(image)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(conf.seed + worker_id)\n",
    "\n",
    "def make_loader(df,\n",
    "                root_dir,\n",
    "                batch_size=conf.batch_size,\n",
    "                shuffle=True,\n",
    "                test=\"train\",\n",
    "                worker_init_fn=worker_init_fn,\n",
    "                **kwargs):\n",
    "\n",
    "    ds = APTOSDataset(\n",
    "        root_dir,\n",
    "        df,\n",
    "        test=test,\n",
    "        **kwargs)\n",
    "\n",
    "    sampler = None\n",
    "    if test == \"train\":\n",
    "        drop_last = True\n",
    "        if conf.weighted_sample:\n",
    "            class_count = df.diagnosis.value_counts()\n",
    "            class_count = 1 / class_count\n",
    "            df['weight'] = df.diagnosis.map(class_count)\n",
    "            sampler = WeightedRandomSampler(df.weight, len(df))\n",
    "    else:\n",
    "        drop_last = False\n",
    "    loader = DataLoader(\n",
    "        ds, batch_size=batch_size, # shuffle=shuffle,\n",
    "        num_workers=conf.num_workers,\n",
    "        sampler=sampler,\n",
    "        drop_last=drop_last)\n",
    "    return loader, len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 arch_name='resnet18',\n",
    "                 input_channel=3,\n",
    "                 input_size=224,\n",
    "                 se=False,\n",
    "                 num_classes=28):\n",
    "        super(ResNet, self).__init__()\n",
    "        if se:\n",
    "            self.base_model = pretrainedmodels.__dict__[arch_name](pretrained=\"imagenet\")\n",
    "        else:\n",
    "            self.base_model = torchvision.models.__dict__[arch_name](pretrained=\"imagenet\")\n",
    "        if isinstance(input_size, tuple):\n",
    "            ksize = (input_size[0] // 16, input_size[1] // 16)\n",
    "        else:\n",
    "            ksize = input_size // 16\n",
    "\n",
    "        self.base_model.bn0 = nn.BatchNorm2d(input_channel)\n",
    "        self.base_model.avgpool = nn.AvgPool2d(kernel_size=ksize)\n",
    "        if se:\n",
    "            self.dim_feats = self.base_model.last_linear.in_features  # = 2048\n",
    "#             self.base_model.last_linear = nn.Linear(self.dim_feats, num_classes)\n",
    "            self.base_model.last_linear = nn.Sequential(\n",
    "            nn.Linear(self.dim_feats, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(p=0.5),            \n",
    "            nn.Linear(32, num_classes))\n",
    "        else:\n",
    "            self.dim_feats = self.base_model.fc.in_features  # = 2048\n",
    "            self.base_model.fc = nn.Linear(self.dim_feats, num_classes)\n",
    "        self.out_size = ksize\n",
    "        self.se = se\n",
    "\n",
    "    def forward(self, data):\n",
    "        # x = self.base_model.bn0(data)\n",
    "        if self.se:\n",
    "            x = self.base_model.layer0(data)\n",
    "        else:\n",
    "            x = self.base_model.conv1(data)\n",
    "            x = self.base_model.bn1(x)\n",
    "            x = self.base_model.relu(x)\n",
    "\n",
    "        x = self.base_model.layer1(x)\n",
    "        x = self.base_model.layer2(x)\n",
    "        x = self.base_model.layer3(x)\n",
    "        x = self.base_model.layer4(x)\n",
    "        x = self.base_model.avgpool(x)\n",
    "        x = x.view(-1, self.dim_feats)\n",
    "        if self.se:\n",
    "            x = self.base_model.last_linear(x)\n",
    "        else:\n",
    "            x = self.base_model.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AptosNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 arch_name='inception_v3',\n",
    "                 input_channel=3,\n",
    "                 input_size=224,\n",
    "                 num_classes=28):\n",
    "        super(AptosNet, self).__init__()\n",
    "        # self.base_model = torchvision.models.__dict__[arch_name](pretrained=\"imagenet\")\n",
    "        self.base_model = pretrainedmodels.__dict__[arch_name](pretrained=\"imagenet\")\n",
    "        \n",
    "        if isinstance(input_size, tuple):\n",
    "            ksize = (input_size[0] // 32, input_size[1] // 32)\n",
    "        else:\n",
    "            ksize = input_size // 32\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=ksize)\n",
    "        self.dim_feats = self.base_model.last_linear.in_features  # = 1024\n",
    "        self.base_model.last_linear = nn.Linear(self.dim_feats, num_classes)\n",
    "        self.out_size = ksize\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.base_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 arch_name='densenet121',\n",
    "                 input_channel=3,\n",
    "                 input_size=224,\n",
    "                 num_classes=28):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.base_model = torchvision.models.__dict__[arch_name](pretrained=\"imagenet\")\n",
    "        \n",
    "        if isinstance(input_size, tuple):\n",
    "            ksize = (input_size[0] // 32, input_size[1] // 32)\n",
    "        else:\n",
    "            ksize = input_size // 32\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=ksize)\n",
    "\n",
    "        self.dim_feats = self.base_model.classifier.in_features  # = 1024\n",
    "        self.base_model.classifier = nn.Linear(self.dim_feats, num_classes)\n",
    "        self.out_size = ksize\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.base_model.features(data)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(-1, self.dim_feats)\n",
    "        x = self.base_model.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLR(_LRScheduler):\n",
    "    \"\"\"SGD with cosine annealing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, step_size_min=1e-5, t0=100, tmult=2, curr_epoch=-1, last_epoch=-1):\n",
    "        self.step_size_min = step_size_min\n",
    "        self.t0 = t0\n",
    "        self.tmult = tmult\n",
    "        self.epochs_since_restart = curr_epoch\n",
    "        super(CosineLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        self.epochs_since_restart += 1\n",
    "\n",
    "        if self.epochs_since_restart > self.t0:\n",
    "            self.t0 *= self.tmult\n",
    "            self.epochs_since_restart = 0\n",
    "\n",
    "        lrs = [self.step_size_min + (\n",
    "                    0.5 * (base_lr - self.step_size_min) * (1 + cos(self.epochs_since_restart * pi / self.t0)))\n",
    "               for base_lr in self.base_lrs]\n",
    "\n",
    "        # print(lrs)\n",
    "\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://inclass.kaggle.com/gennadylaptev/qwk-loss-for-pytorch/data\n",
    "# Categorical Crossentropyから途中で切り替えるのがいいらしい（https://arxiv.org/pdf/1612.00775.pdf）\n",
    "def kappa_loss(p, y, n_classes=5, eps=1e-10):\n",
    "    \"\"\"\n",
    "    QWK loss function as described in https://arxiv.org/pdf/1612.00775.pdf\n",
    "    \n",
    "    Arguments:\n",
    "        p: a tensor with probability predictions, [batch_size, n_classes],\n",
    "        y, a tensor with one-hot encoded class labels, [batch_size, n_classes]\n",
    "    Returns:\n",
    "        QWK loss\n",
    "    \"\"\"\n",
    "    \n",
    "    W = np.zeros((n_classes, n_classes))\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            W[i,j] = (i-j)**2\n",
    "    \n",
    "    W = torch.from_numpy(W.astype(np.float32)).to(conf.device_name)\n",
    "    \n",
    "    p = p.sigmoid()\n",
    "    O = torch.matmul(y.t(), p)\n",
    "    E = torch.matmul(y.sum(dim=0).view(-1,1), p.sum(dim=0).view(1,-1)) / O.sum()\n",
    "    \n",
    "    return (W*O).sum() / ((W*E).sum() + eps)\n",
    "\n",
    "\n",
    "class BCEPlusKappaLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        loss = self.bce_loss(pred, true) + kappa_loss(pred, true, eps=self.eps)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def calc_loss(pred, labels, criterion):\n",
    "    if isinstance(pred, list):\n",
    "        pred_len = len(pred)\n",
    "        pred_probs = 0\n",
    "        clf_loss = 0\n",
    "        for i, px in enumerate(pred):\n",
    "            pred_probs += px.sigmoid().cpu().data.numpy()\n",
    "            clf_loss += criterion(px, labels)\n",
    "        return clf_loss / pred_len, pred_probs / pred_len\n",
    "    else:\n",
    "        pred_probs = pred.sigmoid().cpu().data.numpy()\n",
    "        clf_loss = criterion(pred, labels)\n",
    "        # print(pred, labels, clf_loss)\n",
    "        return clf_loss, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/lextoumbourou/blindness-detection-resnet34-ordinal-targets\n",
    "def get_preds(arr):\n",
    "    mask = arr == 0\n",
    "    return np.clip(np.where(mask.any(1), mask.argmax(1), 5) - 1, 0, 4)\n",
    "\n",
    "def calc_qwk(pred, true):\n",
    "    pred = np.round(pred).reshape(-1)\n",
    "#     pred = np.argmax(pred, axis=1).reshape(-1)\n",
    "#     pred = get_preds(pred > 0.5)\n",
    "#     true  = np.sum(true.astype(int), axis=1) - 1\n",
    "#     print(pred, true)\n",
    "#     true = np.argmax(true, axis=1).reshape(-1)\n",
    "    score = sklearn.metrics.cohen_kappa_score(pred, true,\n",
    "                                          labels=[0,1,2,3,4],\n",
    "                                          weights='quadratic')\n",
    "    return score\n",
    "\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          train_df,\n",
    "          aug,\n",
    "          device,\n",
    "          criterion,\n",
    "          data_dir='../',\n",
    "          undersampling=False):\n",
    "\n",
    "    model.train()\n",
    "    dataloader, ds_size = make_loader(\n",
    "        train_df,\n",
    "        data_dir,\n",
    "        shuffle=True,\n",
    "        test=\"train\",\n",
    "        mixup=conf.mixup,\n",
    "        augment=aug)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_trues = list()\n",
    "    all_preds = list()\n",
    "    sum_loss = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    optimizer.zero_grad()\n",
    "    for i, sample in enumerate(dataloader):\n",
    "        inputs = sample['data'].to(device)\n",
    "        labels = sample['label'].to(device)\n",
    "        all_trues.append(labels.cpu().data.numpy())\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        loss, pred_probs = calc_loss(outputs, labels, criterion)\n",
    "        all_preds.append(pred_probs)\n",
    "        loss.backward()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        if (i + 1) % conf.accum_time == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_trues = np.concatenate(all_trues)\n",
    "    epoch_loss = running_loss / ds_size\n",
    "#     epoch_qwk = calc_qwk(all_preds, all_trues)\n",
    "\n",
    "    result = {'loss': epoch_loss} #, 'qwk': epoch_qwk}\n",
    "    return all_preds, result\n",
    "\n",
    "\n",
    "def validate(model, train_df,\n",
    "             aug,\n",
    "             device,\n",
    "             criterion,\n",
    "             data_dir='../'):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    dataloader, ds_size = make_loader(\n",
    "        train_df,\n",
    "        data_dir,\n",
    "        conf.batch_size,\n",
    "        shuffle=False,\n",
    "        test=\"valid\",\n",
    "        augment=aug)\n",
    "\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for i, samples in enumerate(dataloader):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            inputs = samples['data'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            labels = samples['label'].to(device)\n",
    "\n",
    "            loss, pred_probs = calc_loss(outputs, labels, criterion)\n",
    "#             all_preds.append(pred_probs)\n",
    "            all_preds.append(outputs.cpu().data.numpy())\n",
    "            all_trues.append(labels.cpu().data.numpy())\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_trues = np.concatenate(all_trues)\n",
    "    epoch_qwk = calc_qwk(all_preds, all_trues)\n",
    "\n",
    "    epoch_loss = running_loss / ds_size\n",
    "    result = {'loss': epoch_loss, 'qwk': epoch_qwk}\n",
    "\n",
    "    return all_preds, result\n",
    "\n",
    "def predict(model, test_df,\n",
    "            aug,\n",
    "            device,\n",
    "            data_dir='input/train'):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    dataloader, ds_size = make_loader(\n",
    "        test_df,\n",
    "        data_dir,\n",
    "        conf.batch_size,\n",
    "        shuffle=False,\n",
    "        test=\"test\",\n",
    "        augment=aug)\n",
    "\n",
    "    all_preds = []\n",
    "    # Iterate over data.\n",
    "    t = dataloader\n",
    "    for i, samples in enumerate(t):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            inputs = samples['data'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_preds.append(outputs.cpu().data.numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def mixup_train(model,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          train_df,\n",
    "          aug,\n",
    "          device,\n",
    "          criterion,\n",
    "          data_dir='../',\n",
    "          undersampling=False):\n",
    "\n",
    "    model.train()\n",
    "    dataloader, ds_size = make_loader(\n",
    "        train_df,\n",
    "        data_dir,\n",
    "        shuffle=True,\n",
    "        test=\"train\",\n",
    "        mixup=False,\n",
    "        augment=aug)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over data.\n",
    "    optimizer.zero_grad()\n",
    "    for i, sample in enumerate(tqdm(dataloader)):\n",
    "        inputs = sample['data'].to(device)\n",
    "        labels = sample['label'].to(device)\n",
    "        \n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, conf.alpha)\n",
    "        inputs, targets_a, targets_b = map(torch.autograd.Variable, (inputs, targets_a, targets_b))\n",
    "        outputs = model(inputs)\n",
    "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        loss.backward()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        if (i + 1) % conf.accum_time == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = running_loss / ds_size\n",
    "#     epoch_qwk = calc_qwk(all_preds, all_trues)\n",
    "\n",
    "    result = {'loss': epoch_loss} #, 'qwk': epoch_qwk}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_split(df, val_size=0.2, fold=0):\n",
    "    y = df.diagnosis\n",
    "    mskf = KFold(n_splits=int(1 / val_size), random_state=conf.seed)\n",
    "    splitter = mskf.split(df.id_code, y)\n",
    "    for _ in range(fold + 1):\n",
    "        tr_ind, te_ind = next(splitter)\n",
    "    train_df = df.iloc[tr_ind].reset_index(drop=True)\n",
    "    val_df = df.iloc[te_ind].reset_index(drop=True)\n",
    "    return {'train': train_df, 'val': val_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_df,\n",
    "                test_df,\n",
    "                base_model,\n",
    "                criterion,\n",
    "                log,\n",
    "                device,\n",
    "                fold=0,\n",
    "                num_epoch=1,\n",
    "                mask_epoch=1):\n",
    "\n",
    "    ds = val_split(train_df, fold=fold)\n",
    "    learn_start = time()\n",
    "\n",
    "    log.info('classification learning start')\n",
    "    log.info(\"-\" * 20)\n",
    "    model = base_model.to(device)\n",
    "    # log.info(model)\n",
    "    log.info(f'parameters {count_parameter(model)}')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_qwk = 0\n",
    "    best_clf = 100\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    log.info('Optimizer: Adam')\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=conf.init_lr, weight_decay=1e-5)\n",
    "\n",
    "    log.info(\n",
    "        f\"Scheduler: CosineLR, period={conf.lr_step_epoch}\")\n",
    "#     scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=conf.lr_step_epoch)\n",
    "    # tmax = num_epoch\n",
    "#     scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "#                                                      T_max=conf.lr_step_epoch,\n",
    "#                                                      eta_min=conf.eta_min)\n",
    "    train_ds, val_ds = ds['train'], ds['val']\n",
    "#     train_df = pd.read_csv(conf.all_csv)\n",
    "#     train_df = train_df.loc[train_df.prev == True, train_ds.columns].copy()\n",
    "#     train_ds = pd.concat([train_ds, train_df]).reset_index()\n",
    "#     print(train_ds.shape)\n",
    "#     easy_csv = pd.read_csv(conf.easy_csv)\n",
    "#     train_ds = train_ds[train_ds.id_code.isin(easy_csv.id_code)].copy().reset_index(drop=True)\n",
    "#     print(train_ds.shape)\n",
    "    \n",
    "    scheduler = CosineLR(optimizer, step_size_min=conf.eta_min, \n",
    "                                         t0=len(train_ds) * conf.lr_step_epoch // (conf.batch_size * conf.accum_time), \n",
    "                                         tmult=1)\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        try:\n",
    "            scheduler.step()\n",
    "            start = time()\n",
    "\n",
    "            if conf.mixup_train:\n",
    "                train_res = mixup_train(model, optimizer, scheduler, train_ds, train_transform,\n",
    "                                     device, criterion,\n",
    "                                     data_dir=conf.train_dir)\n",
    "            else:\n",
    "                _, train_res = train(model, optimizer, scheduler, train_ds, train_transform,\n",
    "                                     device, criterion,\n",
    "                                     data_dir=conf.train_dir)\n",
    "\n",
    "            clf_loss = train_res['loss']\n",
    "#             train_qwk = train_res['qwk']\n",
    "\n",
    "            val_preds, val_res = validate(model, val_ds, valid_transform,\n",
    "                                          device, criterion,\n",
    "                                          data_dir=conf.train_dir)\n",
    "            val_clf = val_res['loss']\n",
    "            val_qwk = val_res['qwk']\n",
    "#             val_thr = val_res['thr']\n",
    "\n",
    "            calc_time = time() - start\n",
    "            accum_time = time() - learn_start\n",
    "            lr = get_lr(optimizer)\n",
    "\n",
    "            log_msg = f\"{epoch}\\t{calc_time:.2f}\\t{accum_time:.1f}\\t{lr:.4f}\\t\"\n",
    "            log_msg += f\"{clf_loss:.4f}\\t\" #{train_qwk:.4f}\\t\"\n",
    "            log_msg += f\"{val_clf:.4f}\\t{val_qwk:.4f}\\t\"\n",
    "            log.info(log_msg)\n",
    "\n",
    "            if val_qwk > best_qwk:\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_qwk = val_qwk\n",
    "                best_val_preds = val_preds\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    log.info(\"-\" * 20)\n",
    "    log.info('Best val QWK: {:4f}'.format(best_qwk))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    test_preds = predict(model, test_df, valid_transform,\n",
    "                         device, data_dir=conf.test_dir)\n",
    "\n",
    "    return model, best_val_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "configuration is following...\n",
      "{'accum_time': 4,\n",
      " 'all_csv': '../preprocessed/all.csv',\n",
      " 'alpha': 1,\n",
      " 'batch_size': 16,\n",
      " 'classes_num': 1,\n",
      " 'device_name': 'cuda:0',\n",
      " 'dup_csv': '../preprocessed/strMd5.csv',\n",
      " 'easy_csv': '../preprocessed/easy.csv',\n",
      " 'eta_min': 1e-06,\n",
      " 'fold': 0,\n",
      " 'image_size': 224,\n",
      " 'init_lr': 0.001,\n",
      " 'lr_step_epoch': 64,\n",
      " 'mixup': False,\n",
      " 'mixup_train': False,\n",
      " 'n_splits': 5,\n",
      " 'num_epoch': 128,\n",
      " 'num_workers': 16,\n",
      " 'prev_train_dir': '../previous/previous_train_crop',\n",
      " 'seed': 71,\n",
      " 'test_csv': '../input/sample_submission.csv',\n",
      " 'test_dir': '../preprocessed/test_crop/',\n",
      " 'train_csv': '../input/train.csv',\n",
      " 'train_dir': '../preprocessed/train_crop/',\n",
      " 'weighted_sample': True}\n",
      "done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classification learning start\n",
      "--------------------\n",
      "parameters 10697769\n",
      "Optimizer: Adam\n",
      "Scheduler: CosineLR, period=64\n",
      "0\t77.29\t77.3\t0.0010\t0.8109\t0.3501\t0.8929\t\n",
      "1\t77.01\t154.4\t0.0010\t0.6226\t0.3880\t0.8710\t\n",
      "2\t76.98\t231.4\t0.0010\t0.5169\t0.2838\t0.9010\t\n",
      "3\t77.02\t308.4\t0.0010\t0.4588\t0.3030\t0.8822\t\n",
      "4\t76.98\t385.4\t0.0010\t0.4081\t0.2832\t0.8841\t\n",
      "5\t77.03\t462.4\t0.0010\t0.3920\t0.5303\t0.8299\t\n",
      "6\t77.01\t539.4\t0.0010\t0.3859\t0.2794\t0.8972\t\n",
      "7\t77.04\t616.5\t0.0010\t0.3398\t0.2922\t0.8825\t\n",
      "8\t76.99\t693.5\t0.0010\t0.3255\t0.2785\t0.9034\t\n",
      "9\t77.01\t770.5\t0.0009\t0.2792\t0.3356\t0.8883\t\n",
      "10\t77.05\t847.6\t0.0009\t0.2562\t0.2872\t0.8974\t\n",
      "11\t77.00\t924.6\t0.0009\t0.2642\t0.3901\t0.8838\t\n",
      "12\t77.05\t1001.6\t0.0009\t0.2702\t0.3104\t0.8934\t\n",
      "13\t76.99\t1078.6\t0.0009\t0.2284\t0.3068\t0.9024\t\n",
      "14\t77.02\t1155.6\t0.0009\t0.2191\t0.2685\t0.9052\t\n",
      "15\t77.13\t1232.8\t0.0008\t0.2035\t0.2981\t0.8993\t\n",
      "16\t77.12\t1309.9\t0.0008\t0.2428\t0.2709\t0.9053\t\n",
      "17\t77.09\t1387.0\t0.0008\t0.1921\t0.3060\t0.8932\t\n",
      "18\t77.12\t1464.1\t0.0008\t0.1859\t0.2743\t0.8993\t\n",
      "19\t77.14\t1541.3\t0.0008\t0.1709\t0.2880\t0.9022\t\n",
      "20\t77.09\t1618.4\t0.0007\t0.1607\t0.2585\t0.9080\t\n",
      "21\t77.07\t1695.5\t0.0007\t0.1516\t0.2747\t0.9040\t\n",
      "22\t77.09\t1772.6\t0.0007\t0.1401\t0.2763\t0.9071\t\n",
      "23\t77.07\t1849.6\t0.0007\t0.1446\t0.2599\t0.9058\t\n",
      "24\t77.08\t1926.7\t0.0007\t0.1510\t0.2492\t0.9079\t\n",
      "25\t77.09\t2003.8\t0.0006\t0.1276\t0.2392\t0.9155\t\n",
      "26\t77.12\t2080.9\t0.0006\t0.1348\t0.2456\t0.9066\t\n",
      "27\t77.08\t2158.0\t0.0006\t0.1149\t0.2538\t0.9023\t\n",
      "28\t77.14\t2235.2\t0.0006\t0.1238\t0.2431\t0.9152\t\n",
      "29\t77.12\t2312.3\t0.0005\t0.1379\t0.2442\t0.9204\t\n",
      "30\t77.14\t2389.4\t0.0005\t0.1231\t0.2680\t0.9083\t\n",
      "31\t77.11\t2466.6\t0.0005\t0.1053\t0.2494\t0.9117\t\n",
      "32\t77.18\t2543.7\t0.0005\t0.0994\t0.2554\t0.9162\t\n",
      "33\t77.16\t2620.9\t0.0004\t0.0898\t0.2356\t0.9203\t\n",
      "34\t77.15\t2698.0\t0.0004\t0.0911\t0.2445\t0.9119\t\n",
      "35\t77.16\t2775.2\t0.0004\t0.0838\t0.2554\t0.9132\t\n",
      "36\t77.17\t2852.4\t0.0004\t0.0931\t0.2462\t0.9169\t\n",
      "37\t77.21\t2929.6\t0.0003\t0.0957\t0.2638\t0.9063\t\n",
      "38\t77.16\t3006.7\t0.0003\t0.0824\t0.2535\t0.9095\t\n",
      "39\t77.14\t3083.9\t0.0003\t0.0797\t0.2511\t0.9087\t\n",
      "40\t77.17\t3161.0\t0.0003\t0.0920\t0.2254\t0.9222\t\n",
      "41\t77.18\t3238.2\t0.0002\t0.0766\t0.2368\t0.9108\t\n",
      "42\t77.16\t3315.4\t0.0002\t0.0773\t0.2541\t0.9061\t\n",
      "43\t77.15\t3392.6\t0.0002\t0.0769\t0.2380\t0.9156\t\n",
      "44\t77.14\t3469.7\t0.0002\t0.0671\t0.2449\t0.9108\t\n",
      "45\t77.06\t3546.8\t0.0002\t0.0600\t0.2375\t0.9153\t\n",
      "46\t77.15\t3623.9\t0.0001\t0.0769\t0.2417\t0.9115\t\n",
      "47\t77.12\t3701.0\t0.0001\t0.0667\t0.2399\t0.9113\t\n",
      "48\t77.18\t3778.2\t0.0001\t0.0607\t0.2398\t0.9121\t\n",
      "49\t77.14\t3855.4\t0.0001\t0.0677\t0.2456\t0.9173\t\n",
      "50\t77.15\t3932.5\t0.0001\t0.0580\t0.2392\t0.9155\t\n",
      "51\t77.10\t4009.6\t0.0001\t0.0652\t0.2365\t0.9146\t\n",
      "52\t77.15\t4086.8\t0.0001\t0.0621\t0.2339\t0.9171\t\n",
      "53\t77.15\t4163.9\t0.0000\t0.0590\t0.2347\t0.9190\t\n",
      "54\t77.11\t4241.0\t0.0000\t0.0629\t0.2409\t0.9177\t\n",
      "55\t77.17\t4318.2\t0.0000\t0.0592\t0.2407\t0.9171\t\n",
      "56\t77.13\t4395.3\t0.0000\t0.0573\t0.2420\t0.9173\t\n",
      "57\t77.17\t4472.5\t0.0000\t0.0598\t0.2402\t0.9190\t\n",
      "58\t77.19\t4549.7\t0.0000\t0.0595\t0.2389\t0.9182\t\n",
      "59\t77.12\t4626.8\t0.0000\t0.0618\t0.2394\t0.9197\t\n",
      "60\t77.12\t4703.9\t0.0000\t0.0602\t0.2379\t0.9192\t\n",
      "61\t77.15\t4781.1\t0.0000\t0.0620\t0.2390\t0.9189\t\n",
      "62\t77.19\t4858.3\t0.0010\t0.0617\t0.2556\t0.9091\t\n",
      "63\t77.08\t4935.3\t0.0010\t0.0991\t0.3230\t0.8967\t\n",
      "64\t77.14\t5012.5\t0.0010\t0.1465\t0.3107\t0.8938\t\n",
      "65\t77.14\t5089.6\t0.0010\t0.1489\t0.3891\t0.8654\t\n",
      "66\t77.11\t5166.7\t0.0010\t0.1875\t0.2726\t0.9047\t\n",
      "67\t77.12\t5243.8\t0.0010\t0.1567\t0.2932\t0.8926\t\n",
      "68\t77.19\t5321.0\t0.0010\t0.1328\t0.3311\t0.8806\t\n",
      "69\t77.14\t5398.2\t0.0010\t0.1159\t0.3378\t0.8860\t\n",
      "70\t77.12\t5475.3\t0.0010\t0.1247\t0.2874\t0.9010\t\n",
      "71\t77.11\t5552.4\t0.0009\t0.1334\t0.2990\t0.8977\t\n",
      "72\t77.20\t5629.6\t0.0009\t0.1292\t0.2972\t0.8944\t\n",
      "73\t77.15\t5706.8\t0.0009\t0.1164\t0.2815\t0.9027\t\n",
      "74\t77.21\t5784.0\t0.0009\t0.1126\t0.2704\t0.9009\t\n",
      "75\t77.12\t5861.1\t0.0009\t0.0873\t0.2778\t0.9020\t\n",
      "76\t77.09\t5938.2\t0.0009\t0.0881\t0.2787\t0.9068\t\n",
      "77\t77.16\t6015.4\t0.0009\t0.0949\t0.2666\t0.9005\t\n",
      "78\t77.16\t6092.5\t0.0008\t0.0678\t0.2789\t0.9018\t\n",
      "79\t77.15\t6169.7\t0.0008\t0.0798\t0.2913\t0.8935\t\n",
      "80\t77.14\t6246.8\t0.0008\t0.0849\t0.2917\t0.8973\t\n",
      "81\t77.11\t6323.9\t0.0008\t0.0930\t0.3055\t0.8946\t\n",
      "82\t77.13\t6401.0\t0.0008\t0.0962\t0.2865\t0.9031\t\n",
      "83\t77.15\t6478.2\t0.0007\t0.0806\t0.3026\t0.9021\t\n",
      "84\t77.19\t6555.4\t0.0007\t0.0833\t0.2809\t0.9089\t\n",
      "85\t77.20\t6632.6\t0.0007\t0.0640\t0.2592\t0.9104\t\n",
      "86\t77.17\t6709.8\t0.0007\t0.0573\t0.2786\t0.9097\t\n"
     ]
    }
   ],
   "source": [
    "exp_name = f'prev_plus_{now()}'\n",
    "device, log, result_dir = setup(exp_name, conf)\n",
    "\n",
    "#     train_df = load_csv(conf.train_csv)\n",
    "train_df = pd.read_csv(conf.dup_csv)\n",
    "train_df = train_df[train_df.diagnosis.notnull()].drop_duplicates(subset='strMd5')\n",
    "train_df['diagnosis'] = train_df.diagnosis.astype(int)\n",
    "train_df = train_df[['id_code', 'diagnosis']].copy()\n",
    "#     train_df['prev'] = False\n",
    "test_df = load_csv(conf.test_csv)\n",
    "#     test_df['prev'] = False\n",
    "\n",
    "log.info('done')\n",
    "for i in range(5):\n",
    "    if i != conf.fold:\n",
    "        continue\n",
    "#         model_arch = 'resnet18'\n",
    "#         model_ft = ResNet(arch_name=model_arch, se=False,\n",
    "#                           input_size=conf.image_size,\n",
    "#                           num_classes=conf.classes_num)\n",
    "    model_ft = EfficientNet.from_pretrained('efficientnet-b3', num_classes=conf.classes_num)\n",
    "    model_ft.load_state_dict(torch.load(\"../result/for_pretrained_2019_07_31_11_55_56/model_0.pkl\"))\n",
    "#         for ct, child in enumerate(model_ft.base_model.children()):\n",
    "#             if ct < 6:\n",
    "#                 for param in child.parameters():\n",
    "#                     param.requires_grad = False\n",
    "    criterion = nn.MSELoss()\n",
    "#         criterion = nn.BCEWithLogitsLoss()\n",
    "#         criterion = kappa_loss\n",
    "#         criterion = BCEPlusKappaLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    model_ft, val_preds, test_preds = train_model(\n",
    "        train_df,\n",
    "        test_df,\n",
    "        model_ft,\n",
    "        criterion,\n",
    "        log,\n",
    "        device,\n",
    "        fold=i,\n",
    "        num_epoch=conf.num_epoch)\n",
    "\n",
    "#         for ct, child in enumerate(model_ft.base_model.children()):\n",
    "#             if ct < 6:\n",
    "#                 for param in child.parameters():\n",
    "#                     param.requires_grad = True\n",
    "#         model_ft, val_preds, test_preds = train_model(\n",
    "#             train_df,\n",
    "#             test_df,\n",
    "#             model_ft,\n",
    "#             criterion,\n",
    "#             log,\n",
    "#             device,\n",
    "#             fold=i,\n",
    "#             num_epoch=10)\n",
    "\n",
    "    torch.save(model_ft.state_dict(),  result_dir/f'model_{i}.pkl')\n",
    "    # torch.save(f'model_{i}_all.pkl')\n",
    "    np.save( result_dir/f'val_preds_{i}.npy', val_preds)\n",
    "    np.save( result_dir/f'test_preds_{i}.npy', test_preds)\n",
    "    test_df['diagnosis'] = get_preds(test_preds > 0.4)\n",
    "    print(test_preds)\n",
    "# test_df['diagnosis'] = np.argmax(test_preds, axis=1)\n",
    "#     test_df['diagnosis'] = np.clip(np.round(test_preds), 0, 4).astype(int)\n",
    "    test_df.to_csv(f'submission.csv', index=False)\n",
    "    print(test_df.diagnosis.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
